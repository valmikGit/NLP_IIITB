{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import ast\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # Load the dataset\n",
    "# train_file = \"train_data.csv\"\n",
    "# test_file = \"validation_data.csv\"\n",
    "\n",
    "# train_data = pd.read_csv(train_file)\n",
    "# test_data = pd.read_csv(test_file)\n",
    "\n",
    "# # Data Processing\n",
    "# def process_data(data):\n",
    "#     sentences = []\n",
    "#     for row in data.iloc[:, 0]:  # Assuming the first column has sentence strings\n",
    "#         sentence = ast.literal_eval(row)  # Convert string to list of tuples\n",
    "#         sentences.append(sentence)\n",
    "#     return sentences\n",
    "\n",
    "# train_sentences = process_data(train_data)\n",
    "# test_sentences = process_data(test_data)\n",
    "\n",
    "# # Extract vocabulary and tags\n",
    "# vocab = set(word for sentence in train_sentences for word, _ in sentence)\n",
    "# tags = set(tag for sentence in train_sentences for _, tag in sentence)\n",
    "\n",
    "# # Initialize probability matrices\n",
    "# tag_counts = defaultdict(int)\n",
    "# transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "# emission_counts = defaultdict(lambda: defaultdict(int))\n",
    "# initial_counts = defaultdict(int)\n",
    "\n",
    "# # Compute probabilities\n",
    "# for sentence in train_sentences:\n",
    "#     prev_tag = None\n",
    "#     for word, tag in sentence:\n",
    "#         tag_counts[tag] += 1\n",
    "#         emission_counts[tag][word] += 1\n",
    "\n",
    "#         if prev_tag:\n",
    "#             transition_counts[prev_tag][tag] += 1\n",
    "#         else:\n",
    "#             initial_counts[tag] += 1  # First word tag\n",
    "\n",
    "#         prev_tag = tag\n",
    "\n",
    "# # Convert counts to probabilities\n",
    "# total_sentences = len(train_sentences)\n",
    "# initial_probs = {tag: (initial_counts[tag] + 1) / (total_sentences + len(tags)) for tag in tags}\n",
    "\n",
    "# transition_probs = {tag: {next_tag: (transition_counts[tag][next_tag] + 1) / (tag_counts[tag] + len(tags)) for next_tag in tags} for tag in tags}\n",
    "\n",
    "# emission_probs = {tag: {word: (emission_counts[tag][word] + 1) / (tag_counts[tag] + len(vocab)) for word in vocab} for tag in tags}\n",
    "\n",
    "# # Viterbi Algorithm Implementation\n",
    "# def viterbi(sentence):\n",
    "#     viterbi_matrix = [{}]\n",
    "#     backpointer = [{}]\n",
    "\n",
    "#     for tag in tags:\n",
    "#         viterbi_matrix[0][tag] = initial_probs.get(tag, 1e-6) * emission_probs.get(tag, {}).get(sentence[0], 1e-6)\n",
    "#         backpointer[0][tag] = None\n",
    "\n",
    "#     for i in range(1, len(sentence)):\n",
    "#         viterbi_matrix.append({})\n",
    "#         backpointer.append({})\n",
    "#         for tag in tags:\n",
    "#             max_prob, prev_state = max((viterbi_matrix[i-1][prev_tag] * transition_probs[prev_tag].get(tag, 1e-6) * emission_probs.get(tag, {}).get(sentence[i], 1e-6), prev_tag) for prev_tag in tags)\n",
    "#             viterbi_matrix[i][tag] = max_prob\n",
    "#             backpointer[i][tag] = prev_state\n",
    "\n",
    "#     best_last_tag = max(viterbi_matrix[-1], key=viterbi_matrix[-1].get)\n",
    "#     best_tags = [best_last_tag]\n",
    "\n",
    "#     for i in range(len(sentence)-1, 0, -1):\n",
    "#         best_tags.insert(0, backpointer[i][best_tags[0]])\n",
    "\n",
    "#     return best_tags\n",
    "\n",
    "# # Model Evaluation\n",
    "# actual_tags = []\n",
    "# predicted_tags = []\n",
    "\n",
    "# for sentence in test_sentences:\n",
    "#     words = [word for word, _ in sentence]\n",
    "#     true_tags = [tag for _, tag in sentence]\n",
    "#     pred_tags = viterbi(words)\n",
    "\n",
    "#     actual_tags.extend(true_tags)\n",
    "#     predicted_tags.extend(pred_tags)\n",
    "\n",
    "# # Compute accuracy\n",
    "# accuracy = sum(1 for a, p in zip(actual_tags, predicted_tags) if a == p) / len(actual_tags)\n",
    "# print(f\"Model Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# # Confusion Matrix\n",
    "# conf_matrix = pd.crosstab(pd.Series(actual_tags, name=\"Actual\"), pd.Series(predicted_tags, name=\"Predicted\"))\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "train_file = \"train_data.csv\"\n",
    "test_file = \"validation_data.csv\"\n",
    "\n",
    "train_data = pd.read_csv(train_file)\n",
    "test_data = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       [('DF', 'PROPN')]\n",
      "0      [('03/01/2001', 'NUM'), ('01:35', 'NUM'), ('PM...\n",
      "1      [('The', 'DET'), ('industry', 'NOUN'), ('has',...\n",
      "2      [('Love', 'VERB'), ('this', 'DET'), ('place', ...\n",
      "3      [('This', 'DET'), ('problem', 'NOUN'), ('of', ...\n",
      "4      [('Esso', 'PROPN'), ('said', 'VERB'), ('0', 'X...\n",
      "...                                                  ...\n",
      "52494  [('Superstition', 'NOUN'), ('has', 'VERB'), ('...\n",
      "52495  [('You', 'PRON'), ('now', 'ADV'), ('should', '...\n",
      "52496  [('But', 'CONJ'), ('in', 'ADP'), ('the', 'DET'...\n",
      "52497  [('Specifically', 'ADV'), (',', 'PUNCT'), ('Je...\n",
      "52498  [('B', 'ADJ'), ('&', 'CCONJ'), ('w', 'ADJ'), (...\n",
      "\n",
      "[52499 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'NUM': 17148, 'NOUN': 240200, 'DET': 115786, 'VERB': 151833, 'ADJ': 76692, '.': 94933, 'PUNCT': 23826, 'ADP': 125188, 'CONJ': 27234, 'PROPN': 19556, 'X': 13180, 'PRON': 48875, 'ADV': 49059, 'AUX': 12771, 'INTJ': 755, 'CCONJ': 6535, 'PRT': 18688, 'PART': 7586, 'SCONJ': 3720, '_': 2668, 'SYM': 739})\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of tag vs count.\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "\n",
    "tag_dict = defaultdict(int)\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "    sentence = ast.literal_eval(row.iloc[0])\n",
    "    for word, tag in sentence:\n",
    "        tag_dict[tag] = tag_dict[tag] + 1\n",
    "\n",
    "print(tag_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
