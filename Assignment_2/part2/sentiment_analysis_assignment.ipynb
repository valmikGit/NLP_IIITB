{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1021f1df",
   "metadata": {},
   "source": [
    "\n",
    "# Step 1: Understand the Dataset\n",
    "\n",
    "## 1. Load the Dataset\n",
    "We load the sentiment analysis dataset using pandas. The dataset contains two columns: **Text** and **Sentiment**. Each row represents a sentence (or review) and its corresponding sentiment label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9034862c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sentiment values before categorization: ['Positive' 'Negative' 'Neutral' 'Anger' 'Fear' 'Sadness' 'Disgust'\n",
      " 'Happiness' 'Joy' 'Love' 'Amusement' 'Enjoyment' 'Admiration' 'Affection'\n",
      " 'Awe' 'Disappointed' 'Surprise' 'Acceptance' 'Adoration' 'Anticipation'\n",
      " 'Bitter' 'Calmness' 'Confusion' 'Excitement' 'Kind' 'Pride' 'Shame'\n",
      " 'Elation' 'Euphoria' 'Contentment' 'Serenity' 'Gratitude' 'Hope'\n",
      " 'Empowerment' 'Compassion' 'Tenderness' 'Arousal' 'Enthusiasm'\n",
      " 'Fulfillment' 'Reverence' 'Despair' 'Grief' 'Loneliness' 'Jealousy'\n",
      " 'Resentment' 'Frustration' 'Boredom' 'Anxiety' 'Intimidation'\n",
      " 'Helplessness' 'Envy' 'Regret' 'Curiosity' 'Indifference' 'Numbness'\n",
      " 'Melancholy' 'Nostalgia' 'Ambivalence' 'Determination' 'Zest' 'Hopeful'\n",
      " 'Proud' 'Grateful' 'Empathetic' 'Compassionate' 'Playful' 'Free-spirited'\n",
      " 'Inspired' 'Confident' 'Bitterness' 'Yearning' 'Fearful' 'Apprehensive'\n",
      " 'Overwhelmed' 'Jealous' 'Devastated' 'Frustrated' 'Envious' 'Dismissive'\n",
      " 'Thrill' 'Bittersweet' 'Overjoyed' 'Inspiration' 'Motivation'\n",
      " 'Contemplation' 'JoyfulReunion' 'Satisfaction' 'Blessed' 'Reflection'\n",
      " 'Appreciation' 'Confidence' 'Accomplishment' 'Wonderment' 'Optimism'\n",
      " 'Enchantment' 'Intrigue' 'PlayfulJoy' 'Mindfulness' 'DreamChaser'\n",
      " 'Elegance' 'Whimsy' 'Pensive' 'Harmony' 'Creativity' 'Radiance' 'Wonder'\n",
      " 'Rejuvenation' 'Coziness' 'Adventure' 'Melodic' 'FestiveJoy'\n",
      " 'InnerJourney' 'Freedom' 'Dazzle' 'Adrenaline' 'ArtisticBurst'\n",
      " 'CulinaryOdyssey' 'Resilience' 'Immersion' 'Spark' 'Marvel' 'Heartbreak'\n",
      " 'Betrayal' 'Suffering' 'EmotionalStorm' 'Isolation' 'Disappointment'\n",
      " 'LostLove' 'Exhaustion' 'Sorrow' 'Darkness' 'Desperation' 'Ruins'\n",
      " 'Desolation' 'Loss' 'Heartache' 'Solitude' 'Positivity' 'Kindness'\n",
      " 'Friendship' 'Success' 'Exploration' 'Amazement' 'Romance' 'Captivation'\n",
      " 'Tranquility' 'Grandeur' 'Emotion' 'Energy' 'Celebration' 'Charm'\n",
      " 'Ecstasy' 'Colorful' 'Hypnotic' 'Connection' 'Iconic' 'Journey'\n",
      " 'Engagement' 'Touched' 'Suspense' 'Triumph' 'Heartwarming' 'Obstacle'\n",
      " 'Sympathy' 'Pressure' 'Renewed Effort' 'Miscalculation' 'Challenge'\n",
      " 'Solace' 'Breakthrough' 'Joy in Baking' 'Envisioning History'\n",
      " 'Imagination' 'Vibrancy' 'Mesmerizing' 'Culinary Adventure'\n",
      " 'Winter Magic' 'Thrilling Journey' \"Nature's Beauty\" 'Celestial Wonder'\n",
      " 'Creative Inspiration' 'Runway Creativity' \"Ocean's Freedom\"\n",
      " 'Whispers of the Past' 'Relief' 'Embarrassed' 'Mischievous' 'Sad' 'Hate'\n",
      " 'Bad' 'Happy']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"sentimentdataset.csv\")\n",
    "df = df[[\"Text\", \"Sentiment\"]].dropna()\n",
    "df[\"Sentiment\"] = df[\"Sentiment\"].str.strip()\n",
    "\n",
    "# Check unique sentiment classes before categorization\n",
    "print(\"Unique sentiment values before categorization:\", df[\"Sentiment\"].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6559b469",
   "metadata": {},
   "source": [
    "\n",
    "# Step 2: Preprocessing\n",
    "\n",
    "## 1. Text Cleaning and Normalization\n",
    "We preprocess the text data by converting it to lowercase, removing punctuation and stopwords, and applying stemming and lemmatization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dd2f528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Text Preprocessing Function\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(stemmer.stem(word)) for word in words if word not in stop_words]\n",
    "    return ' '.join(words).strip()\n",
    "\n",
    "# Apply preprocessing\n",
    "df[\"ProcessedText\"] = df[\"Text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acec139",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Sentiment Categorization with spaCy Embeddings\n",
    "We use spaCy's pre-trained word embeddings to map raw sentiment labels into one of three standardized sentiment categories: **Positive**, **Negative**, and **Neutral**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a5f3bc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load spaCy model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_md\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Reference words for sentiment categories\u001b[39;00m\n\u001b[0;32m      9\u001b[0m ref_words \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositive\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhappy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgood\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexcellent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamazing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNegative\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msad\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbad\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterrible\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhorrible\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mawful\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeutral\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mokay\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmoderate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindifferent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     13\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\krish\\OneDrive-MSFT\\Subjects6thSemester\\NLP\\Assignments\\NLP_IIITB\\assign-1_NLP\\Lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krish\\OneDrive-MSFT\\Subjects6thSemester\\NLP\\Assignments\\NLP_IIITB\\assign-1_NLP\\Lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Reference words for sentiment categories\n",
    "ref_words = {\n",
    "    \"Positive\": [\"happy\", \"great\", \"good\", \"excellent\", \"amazing\", \"positive\"],\n",
    "    \"Negative\": [\"sad\", \"bad\", \"terrible\", \"horrible\", \"negative\", \"awful\"],\n",
    "    \"Neutral\": [\"okay\", \"neutral\", \"average\", \"moderate\", \"indifferent\"]\n",
    "}\n",
    "\n",
    "# Compute average vector for each category\n",
    "def compute_average_vector(words):\n",
    "    vectors = [nlp(word).vector for word in words if nlp(word).has_vector]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros((nlp(\"positive\").vector.shape[0],))\n",
    "\n",
    "ref_vectors = {category: compute_average_vector(words) for category, words in ref_words.items()}\n",
    "\n",
    "# Normalize reference vectors\n",
    "for category in ref_vectors:\n",
    "    ref_vectors[category] /= np.linalg.norm(ref_vectors[category])\n",
    "\n",
    "# Assign sentiment based on cosine similarity\n",
    "def assign_sentiment_category(sentiment):\n",
    "    doc = nlp(sentiment)\n",
    "    if not doc.has_vector:\n",
    "        return \"Neutral\"\n",
    "    word_vector = doc.vector / np.linalg.norm(doc.vector)\n",
    "    similarities = {category: cosine_similarity([word_vector], [ref_vec])[0][0] for category, ref_vec in ref_vectors.items()}\n",
    "    return max(similarities, key=similarities.get)\n",
    "\n",
    "# Apply sentiment classification\n",
    "df[\"Sentiment\"] = df[\"Sentiment\"].apply(assign_sentiment_category)\n",
    "print(\"Unique sentiment values after categorization:\", df[\"Sentiment\"].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff41394",
   "metadata": {},
   "source": [
    "\n",
    "# Step 3: Define Neural Network Architecture\n",
    "\n",
    "## Label Encoding and Vectorization\n",
    "We encode sentiment labels and convert the processed text into TF-IDF features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f663fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"SentimentEncoded\"] = label_encoder.fit_transform(df[\"Sentiment\"])\n",
    "\n",
    "# Split dataset\n",
    "X = df[\"ProcessedText\"]\n",
    "y = df[\"SentimentEncoded\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=30000, min_df=3, stop_words=\"english\", sublinear_tf=True)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc17ec35",
   "metadata": {},
   "source": [
    "\n",
    "## Choice of Architecture\n",
    "We use **MLPClassifier** with two hidden layers (100 neurons each) and ReLU activation. The choice of this architecture balances complexity and generalization, with dropout for regularization in DNN. Loss function is **categorical cross-entropy**, suitable for multi-class classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1357e168",
   "metadata": {},
   "source": [
    "\n",
    "# Step 4: Train and Validate the Model\n",
    "\n",
    "## Handle Class Imbalance with SMOTE\n",
    "We apply SMOTE (Synthetic Minority Oversampling Technique) to balance classes in training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f471c4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE\n",
    "from collections import Counter\n",
    "min_class_size = Counter(y_train).most_common()[-1][1]\n",
    "k_neighbors = min(5, min_class_size - 1) if min_class_size > 1 else 1\n",
    "smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "X_train_tfidf, y_train = smote.fit_resample(X_train_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f071db",
   "metadata": {},
   "source": [
    "\n",
    "## Train MLP Classifier\n",
    "We use the Adam optimizer with a constant learning rate. The model is trained for 500 iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48499ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train MLP Classifier\n",
    "best_mlp = MLPClassifier(activation='relu', alpha=0.001, batch_size=32,\n",
    "                         hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
    "                         max_iter=500, solver='adam', random_state=42)\n",
    "best_mlp.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = best_mlp.predict(X_test_tfidf)\n",
    "print(f\"MLP Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"MLP Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b362a8ee",
   "metadata": {},
   "source": [
    "\n",
    "# Step 5: Evaluate the Model\n",
    "\n",
    "## Train Deep Neural Network (DNN)\n",
    "We train a DNN with multiple dense layers and dropout. The model is compiled using the Adam optimizer and evaluated with accuracy and classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4b71a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Prepare labels\n",
    "y_train_dnn = to_categorical(y_train, num_classes=3)\n",
    "y_test_dnn = to_categorical(y_test, num_classes=3)\n",
    "\n",
    "# Build DNN\n",
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(X_train_tfidf.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train DNN\n",
    "model.fit(X_train_tfidf.toarray(), y_train_dnn, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate DNN\n",
    "y_pred_dnn = np.argmax(model.predict(X_test_tfidf.toarray()), axis=1)\n",
    "print(\"DNN Accuracy:\", accuracy_score(y_test, y_pred_dnn))\n",
    "print(\"DNN Classification Report:\\n\", classification_report(y_test, y_pred_dnn))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assign-1_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
