{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "train_file = \"train_data.csv\"\n",
    "test_file = \"validation_data.csv\"\n",
    "\n",
    "train_data = pd.read_csv(train_file)\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "# Dictionary for tag vs count\n",
    "tag_dict = defaultdict(int)\n",
    "\n",
    "# Dictionary for {word, tag} vs count\n",
    "word_tag_dict = defaultdict(int)\n",
    "\n",
    "# Dictionary for {tag i, tag i + 1} vs count\n",
    "two_tag_dict = defaultdict(int)\n",
    "\n",
    "# Dictionary of first word vs count\n",
    "first_word_dict = defaultdict(int)\n",
    "\n",
    "# Total sentence count\n",
    "sentence_count = 0\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "    sentence = row.iloc[0]\n",
    "\n",
    "    # Ensure the sentence is a list, otherwise try converting\n",
    "    if isinstance(sentence, str):\n",
    "        try:\n",
    "            sentence = ast.literal_eval(sentence)\n",
    "        except (ValueError, SyntaxError):\n",
    "            print(f\"Skipping row {index}: Invalid format -> {sentence}\")\n",
    "            continue  # Skip this row\n",
    "\n",
    "    if not isinstance(sentence, list):\n",
    "        print(f\"Skipping row {index}: Not a list -> {sentence}\")\n",
    "        continue\n",
    "    \n",
    "    sentence_count += 1\n",
    "    \n",
    "    # Check if the length of the sentence is more than zero\n",
    "    if len(sentence) > 0:\n",
    "        first_word_dict[(lemmatizer.lemmatize(sentence[0][0]), sentence[0][1])] += 1\n",
    "\n",
    "    prev_tag = None\n",
    "    for word, tag in sentence:\n",
    "        lemmatized_word = lemmatizer.lemmatize(word)  # Apply lemmatization\n",
    "        tag_dict[tag] += 1\n",
    "        word_tag_dict[(lemmatized_word, tag)] += 1\n",
    "\n",
    "        if prev_tag is not None:\n",
    "            two_tag_dict[(prev_tag, tag)] += 1\n",
    "        prev_tag = tag\n",
    "\n",
    "print(sentence_count)\n",
    "\n",
    "# Initial probability calculation\n",
    "initial_probability = defaultdict(int)\n",
    "for key, value in first_word_dict.items():\n",
    "    initial_probability[key[1]] += value\n",
    "\n",
    "for key, value in initial_probability.items():\n",
    "    initial_probability[key] = value/sentence_count\n",
    "\n",
    "# Emission probability calculation\n",
    "emission_probability = defaultdict(lambda: defaultdict(int))\n",
    "for key, value in word_tag_dict.items():\n",
    "    emission_probability[key[1]][key[0]] = value/tag_dict[key[1]]\n",
    "\n",
    "# Transition probability calculation\n",
    "transition_probability = defaultdict(lambda: defaultdict(int))\n",
    "for key, value in two_tag_dict.items():\n",
    "    transition_probability[key[0]][key[1]] = value/tag_dict[key[0]]\n",
    "\n",
    "# Viterbi algorithm\n",
    "def viterbi_algorithm(sentence:list, unique_tags:list, initial_prob:defaultdict, transition_prob:defaultdict, emission_prob:defaultdict):\n",
    "    n = len(sentence)  # Number of words\n",
    "    tags_list = list(unique_tags)  # Convert set to list for indexing\n",
    "\n",
    "    # Initialize Viterbi and backpointer as a list of dictionaries\n",
    "    viterbi = [{} for _ in range(n)]\n",
    "    backpointer = [{} for _ in range(n)]\n",
    "\n",
    "    # Step 1: Initialization\n",
    "    for tag in tags_list:\n",
    "        viterbi[0][tag] = initial_prob.get(tag, 0) * emission_prob.get(tag, {}).get(sentence[0], 0)\n",
    "        backpointer[0][tag] = None\n",
    "\n",
    "    # Step 2: Recursion\n",
    "    for t in range(1, n):  # Iterate over words\n",
    "        for curr_tag in tags_list:\n",
    "            max_prob, best_prev_tag = max(\n",
    "                (viterbi[t - 1].get(prev_tag, 0) * transition_prob.get(prev_tag, {}).get(curr_tag, 0) * emission_prob.get(curr_tag, {}).get(sentence[t], 0), prev_tag)\n",
    "                for prev_tag in tags_list\n",
    "            )\n",
    "            viterbi[t][curr_tag] = max_prob\n",
    "            backpointer[t][curr_tag] = best_prev_tag\n",
    "\n",
    "    # Step 3: Termination\n",
    "    best_tags = []\n",
    "    best_last_tag = max(tags_list, key=lambda tag: viterbi[-1].get(tag, 0))\n",
    "    best_tags.append(best_last_tag)\n",
    "\n",
    "    # Step 4: Backtracking\n",
    "    for t in range(n - 1, 0, -1):\n",
    "        best_last_tag = backpointer[t][best_last_tag]\n",
    "        best_tags.insert(0, best_last_tag)\n",
    "\n",
    "    return best_tags\n",
    "\n",
    "# Prepare test sentences\n",
    "predicted_tags = []\n",
    "actual_tags = []\n",
    "\n",
    "# Convert string representation of lists into actual lists\n",
    "validation_sentences = []\n",
    "for _, row in test_data.iterrows():\n",
    "    try:\n",
    "        sentence = ast.literal_eval(row.iloc[0])  # Convert string to list of (word, tag) tuples\n",
    "        if isinstance(sentence, list) and len(sentence) > 0:\n",
    "            validation_sentences.append([(lemmatizer.lemmatize(word), tag) for word, tag in sentence])\n",
    "    except (SyntaxError, ValueError):\n",
    "        continue  # Skip invalid rows\n",
    "\n",
    "# Check if valid sentences exist\n",
    "if not validation_sentences:\n",
    "    print(\"No valid test sentences found. Please check your dataset.\")\n",
    "    exit()\n",
    "\n",
    "# Extract list of tags\n",
    "tags_list = list(tag_dict.keys())\n",
    "\n",
    "# Run Viterbi on test sentences\n",
    "for sentence in validation_sentences:\n",
    "    words = [word for word, _ in sentence]\n",
    "    \n",
    "    if not words:  # Skip empty sentences\n",
    "        continue\n",
    "    \n",
    "    actual_tags.extend([tag for _, tag in sentence])\n",
    "    predicted_tags.extend(\n",
    "        viterbi_algorithm(\n",
    "            words,\n",
    "            tags_list,\n",
    "            initial_probability,\n",
    "            transition_probability,\n",
    "            emission_probability\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(actual_tags, predicted_tags)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "unique_tags = list(tag_dict.keys())\n",
    "cm = confusion_matrix(actual_tags, predicted_tags, labels=unique_tags)\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=unique_tags, yticklabels=unique_tags, cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Tag\")\n",
    "plt.ylabel(\"Actual Tag\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
