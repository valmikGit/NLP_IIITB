{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "train_data = pd.read_csv(\"train_data.csv\")\n",
    "test_data = pd.read_csv(\"validation_data.csv\")\n",
    "\n",
    "# Function to process each row with lemmatization\n",
    "def process_text(text):\n",
    "    try:\n",
    "        tokens = ast.literal_eval(text)  # Convert string representation of list to actual list\n",
    "        processed_tokens = [\n",
    "            (lemmatizer.lemmatize(word) if tag not in ['PRON', 'PROPN', 'NOUN'] else word, 'PUNCT' if word in string.punctuation else tag)\n",
    "            for word, tag in tokens\n",
    "        ]\n",
    "        return str(processed_tokens)  # Convert back to string format\n",
    "    except Exception as e:\n",
    "        return text  # Return original if any error occurs\n",
    "\n",
    "# Identify the column name dynamically\n",
    "train_col = train_data.columns[0]\n",
    "test_col = test_data.columns[0]\n",
    "\n",
    "# Apply transformation\n",
    "train_data[train_col] = train_data[train_col].apply(process_text)\n",
    "test_data[test_col] = test_data[test_col].apply(process_text)\n",
    "\n",
    "# Dictionary for tag vs count\n",
    "tag_dict = defaultdict(int)\n",
    "word_tag_dict = defaultdict(int)\n",
    "two_tag_dict = defaultdict(int)\n",
    "first_word_dict = defaultdict(int)\n",
    "\n",
    "# Total sentence count\n",
    "sentence_count = 0\n",
    "for index, row in train_data.iterrows():\n",
    "    sentence = row.iloc[0]\n",
    "    if isinstance(sentence, str):\n",
    "        try:\n",
    "            sentence = ast.literal_eval(sentence)\n",
    "        except (ValueError, SyntaxError):\n",
    "            continue  # Skip invalid rows\n",
    "    if not isinstance(sentence, list):\n",
    "        continue\n",
    "    \n",
    "    sentence_count += 1\n",
    "    if len(sentence) > 0:\n",
    "        first_word_dict[(sentence[0][0], sentence[0][1])] += 1\n",
    "\n",
    "    prev_tag = None\n",
    "    for word, tag in sentence:\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        tag_dict[tag] += 1\n",
    "        word_tag_dict[(word, tag)] += 1\n",
    "        if prev_tag is not None:\n",
    "            two_tag_dict[(prev_tag, tag)] += 1\n",
    "        prev_tag = tag\n",
    "\n",
    "# Initial probability calculation\n",
    "initial_probability = defaultdict(int)\n",
    "for key, value in first_word_dict.items():\n",
    "    initial_probability[key[1]] += value\n",
    "for key, value in initial_probability.items():\n",
    "    initial_probability[key] = value/sentence_count\n",
    "\n",
    "# Emission probability calculation\n",
    "emission_probability = defaultdict(lambda: defaultdict(int))\n",
    "for key, value in word_tag_dict.items():\n",
    "    emission_probability[key[1]][key[0]] = value/tag_dict[key[1]]\n",
    "\n",
    "# Transition probability calculation\n",
    "transition_probability = defaultdict(lambda: defaultdict(int))\n",
    "for key, value in two_tag_dict.items():\n",
    "    transition_probability[key[0]][key[1]] = value/tag_dict[key[0]]\n",
    "\n",
    "# Viterbi algorithm\n",
    "def viterbi_algorithm(sentence:list, unique_tags:list, initial_prob:defaultdict, transition_prob:defaultdict, emission_prob:defaultdict):\n",
    "    n = len(sentence)\n",
    "    tags_list = list(unique_tags)\n",
    "    viterbi = [{} for _ in range(n)]\n",
    "    backpointer = [{} for _ in range(n)]\n",
    "    for tag in tags_list:\n",
    "        viterbi[0][tag] = initial_prob.get(tag, 0) * emission_prob.get(tag, {}).get(sentence[0], 0)\n",
    "        backpointer[0][tag] = None\n",
    "    for t in range(1, n):\n",
    "        for curr_tag in tags_list:\n",
    "            max_prob, best_prev_tag = max(\n",
    "                (viterbi[t - 1].get(prev_tag, 0) * transition_prob.get(prev_tag, {}).get(curr_tag, 0) * emission_prob.get(curr_tag, {}).get(sentence[t], 0), prev_tag)\n",
    "                for prev_tag in tags_list\n",
    "            )\n",
    "            viterbi[t][curr_tag] = max_prob\n",
    "            backpointer[t][curr_tag] = best_prev_tag\n",
    "    best_tags = []\n",
    "    best_last_tag = max(tags_list, key=lambda tag: viterbi[-1].get(tag, 0))\n",
    "    best_tags.append(best_last_tag)\n",
    "    for t in range(n - 1, 0, -1):\n",
    "        best_last_tag = backpointer[t][best_last_tag]\n",
    "        best_tags.insert(0, best_last_tag)\n",
    "    return best_tags\n",
    "\n",
    "# Prepare test sentences\n",
    "predicted_tags = []\n",
    "actual_tags = []\n",
    "validation_sentences = []\n",
    "for _, row in test_data.iterrows():\n",
    "    try:\n",
    "        sentence = ast.literal_eval(row.iloc[0])\n",
    "        if isinstance(sentence, list) and len(sentence) > 0:\n",
    "            validation_sentences.append([(lemmatizer.lemmatize(word), tag) for word, tag in sentence])\n",
    "    except (SyntaxError, ValueError):\n",
    "        continue\n",
    "if not validation_sentences:\n",
    "    print(\"No valid test sentences found. Please check your dataset.\")\n",
    "    exit()\n",
    "tags_list = list(tag_dict.keys())\n",
    "for sentence in validation_sentences:\n",
    "    words = [word for word, _ in sentence]\n",
    "    if not words:\n",
    "        continue\n",
    "    actual_tags.extend([tag for _, tag in sentence])\n",
    "    predicted_tags.extend(\n",
    "        viterbi_algorithm(\n",
    "            words,\n",
    "            tags_list,\n",
    "            initial_probability,\n",
    "            transition_probability,\n",
    "            emission_probability\n",
    "        )\n",
    "    )\n",
    "accuracy = accuracy_score(actual_tags, predicted_tags)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "cm = confusion_matrix(actual_tags, predicted_tags, labels=list(tag_dict.keys()))\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=tag_dict.keys(), yticklabels=tag_dict.keys(), cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Tag\")\n",
    "plt.ylabel(\"Actual Tag\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
