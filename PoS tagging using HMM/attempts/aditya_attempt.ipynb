{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T18:48:18.230842Z",
     "iopub.status.busy": "2025-02-07T18:48:18.230496Z",
     "iopub.status.idle": "2025-02-07T18:48:18.844160Z",
     "shell.execute_reply": "2025-02-07T18:48:18.843080Z",
     "shell.execute_reply.started": "2025-02-07T18:48:18.230814Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "\n",
    "# train_file = \"/kaggle/input/nlp-project/TRAIN - TRAIN.csv.csv\"\n",
    "# test_file = \"/kaggle/input/nlp-project/TEST - TEST.csv.csv\"\n",
    "\n",
    "train_data = pd.read_csv(\"train_data.csv\")\n",
    "test_data = pd.read_csv(\"validation_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T18:48:18.845272Z",
     "iopub.status.busy": "2025-02-07T18:48:18.845025Z",
     "iopub.status.idle": "2025-02-07T18:48:18.849169Z",
     "shell.execute_reply": "2025-02-07T18:48:18.848283Z",
     "shell.execute_reply.started": "2025-02-07T18:48:18.845248Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import ast\n",
    "\n",
    "# # Function to process each row\n",
    "# def process_text(text):\n",
    "#     try:\n",
    "#         tokens = ast.literal_eval(text)  # Convert string representation of list to actual list\n",
    "#         processed_tokens = [\n",
    "#             (word if tag in ['PRON', 'PROPN', 'NOUN'] else word.lower(), tag)\n",
    "#             for word, tag in tokens\n",
    "#         ]\n",
    "#         return str(processed_tokens)  # Convert back to string format\n",
    "#     except Exception as e:\n",
    "#         return text  # Return original if any error occurs\n",
    "\n",
    "# # Identify the column name dynamically\n",
    "# train_col = train_data.columns[0]\n",
    "# test_col = test_data.columns[0]\n",
    "\n",
    "# # Apply transformation\n",
    "# train_data[train_col] = train_data[train_col].apply(process_text)\n",
    "# test_data[test_col] = test_data[test_col].apply(process_text)\n",
    "\n",
    "# # Display processed data\n",
    "# train_data.head(), test_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T18:48:18.850485Z",
     "iopub.status.busy": "2025-02-07T18:48:18.850257Z",
     "iopub.status.idle": "2025-02-07T18:48:34.287578Z",
     "shell.execute_reply": "2025-02-07T18:48:34.286449Z",
     "shell.execute_reply.started": "2025-02-07T18:48:18.850463Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import string\n",
    "\n",
    "# Function to process each row\n",
    "def process_text(text):\n",
    "    try:\n",
    "        tokens = ast.literal_eval(text)  # Convert string representation of list to actual list\n",
    "        processed_tokens = [\n",
    "            (word if tag in ['PRON', 'PROPN', 'NOUN'] else word.lower(), 'PUNCT' if word in string.punctuation else tag)\n",
    "            for word, tag in tokens\n",
    "        ]\n",
    "        return str(processed_tokens)  # Convert back to string format\n",
    "    except Exception as e:\n",
    "        return text  # Return original if any error occurs\n",
    "\n",
    "# Identify the column name dynamically\n",
    "train_col = train_data.columns[0]\n",
    "test_col = test_data.columns[0]\n",
    "\n",
    "# Apply transformation\n",
    "train_data[train_col] = train_data[train_col].apply(process_text)\n",
    "test_data[test_col] = test_data[test_col].apply(process_text)\n",
    "\n",
    "# Display processed data\n",
    "train_data.head(), test_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T18:48:34.288892Z",
     "iopub.status.busy": "2025-02-07T18:48:34.288623Z",
     "iopub.status.idle": "2025-02-07T18:48:34.292386Z",
     "shell.execute_reply": "2025-02-07T18:48:34.291209Z",
     "shell.execute_reply.started": "2025-02-07T18:48:34.288868Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T18:48:34.293544Z",
     "iopub.status.busy": "2025-02-07T18:48:34.293273Z",
     "iopub.status.idle": "2025-02-07T18:48:34.303993Z",
     "shell.execute_reply": "2025-02-07T18:48:34.302939Z",
     "shell.execute_reply.started": "2025-02-07T18:48:34.293519Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import ast\n",
    "# import string\n",
    "# import nltk\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# # Initialize lemmatizer\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# # Function to convert words to lowercase (except PRON, PROPN, NOUN)\n",
    "# def convert_to_lowercase(word, tag):\n",
    "#     return word if tag in ['PRON', 'PROPN', 'NOUN'] else word.lower()\n",
    "\n",
    "# # Function to replace punctuation tags\n",
    "# def handle_punctuation(word, tag):\n",
    "#     if all(char in string.punctuation for char in word):  \n",
    "#         return word, 'PUNCT'  # Keep word unchanged, update tag\n",
    "#     return word, tag  # Return as is if not punctuation\n",
    "\n",
    "# # Function to apply lemmatization\n",
    "# def apply_lemmatization(word):\n",
    "#     return lemmatizer.lemmatize(word)\n",
    "\n",
    "# # Main function to process each row\n",
    "# def process_text(text):\n",
    "#     try:\n",
    "#         tokens = ast.literal_eval(text)  # Convert string representation of list to actual list\n",
    "#         processed_tokens = []\n",
    "\n",
    "#         for word, tag in tokens:\n",
    "#             word, tag = handle_punctuation(word, tag)  # Handle punctuation\n",
    "#             word = convert_to_lowercase(word, tag)  # Convert to lowercase if needed\n",
    "#             word = apply_lemmatization(word)  # Apply lemmatization\n",
    "\n",
    "#             processed_tokens.append((word, tag))\n",
    "\n",
    "#         return str(processed_tokens)  # Convert back to string format\n",
    "#     except Exception as e:\n",
    "#         return text  # Return original if any error occurs\n",
    "\n",
    "# # Identify the column name dynamically\n",
    "# train_col = train_data.columns[0]\n",
    "# test_col = test_data.columns[0]\n",
    "\n",
    "# # Apply transformation\n",
    "# train_data[train_col] = train_data[train_col].apply(process_text)\n",
    "# test_data[test_col] = test_data[test_col].apply(process_text)\n",
    "\n",
    "# # Display processed data\n",
    "# train_data.head(), test_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T18:48:34.305560Z",
     "iopub.status.busy": "2025-02-07T18:48:34.305263Z",
     "iopub.status.idle": "2025-02-07T18:48:45.404645Z",
     "shell.execute_reply": "2025-02-07T18:48:45.403364Z",
     "shell.execute_reply.started": "2025-02-07T18:48:34.305536Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dictionary for tag vs count\n",
    "tag_dict = defaultdict(int)\n",
    "\n",
    "# Dictionary for {word, tag} vs count\n",
    "word_tag_dict = defaultdict(int)\n",
    "\n",
    "# Dictionary for {tag i, tag i + 1} vs count\n",
    "two_tag_dict = defaultdict(int)\n",
    "\n",
    "# Dictionary of first word vs count\n",
    "first_word_dict = defaultdict(int)\n",
    "\n",
    "# Total sentence count\n",
    "sentence_count = 0\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "    sentence = row.iloc[0]\n",
    "\n",
    "    # Ensure the sentence is a list, otherwise try converting\n",
    "    if isinstance(sentence, str):\n",
    "        try:\n",
    "            sentence = ast.literal_eval(sentence)\n",
    "        except (ValueError, SyntaxError):\n",
    "            print(f\"Skipping row {index}: Invalid format -> {sentence}\")\n",
    "            continue  # Skip this row\n",
    "\n",
    "    if not isinstance(sentence, list):\n",
    "        print(f\"Skipping row {index}: Not a list -> {sentence}\")\n",
    "        continue\n",
    "    \n",
    "    sentence_count += 1\n",
    "    \n",
    "    # Check if the length of the sentence is more than zero\n",
    "    if len(sentence) > 0:\n",
    "        first_word_dict[(sentence[0][0], sentence[0][1])] += 1\n",
    "\n",
    "    prev_tag = None\n",
    "    for word, tag in sentence:\n",
    "        tag_dict[tag] += 1\n",
    "        word_tag_dict[(word, tag)] += 1\n",
    "\n",
    "        if prev_tag is not None:\n",
    "            two_tag_dict[(prev_tag, tag)] += 1\n",
    "        prev_tag = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T18:48:45.405669Z",
     "iopub.status.busy": "2025-02-07T18:48:45.405428Z",
     "iopub.status.idle": "2025-02-07T18:48:45.408801Z",
     "shell.execute_reply": "2025-02-07T18:48:45.407834Z",
     "shell.execute_reply.started": "2025-02-07T18:48:45.405645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(sentence_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T18:48:45.410062Z",
     "iopub.status.busy": "2025-02-07T18:48:45.409683Z",
     "iopub.status.idle": "2025-02-07T18:48:45.423700Z",
     "shell.execute_reply": "2025-02-07T18:48:45.422910Z",
     "shell.execute_reply.started": "2025-02-07T18:48:45.410038Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initial probability calculation\n",
    "initial_probability = defaultdict(int)\n",
    "for key, value in first_word_dict.items():\n",
    "    initial_probability[key[1]] += value\n",
    "\n",
    "for key, value in initial_probability.items():\n",
    "    initial_probability[key] = value/sentence_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T18:48:45.424560Z",
     "iopub.status.busy": "2025-02-07T18:48:45.424371Z",
     "iopub.status.idle": "2025-02-07T18:48:45.431939Z",
     "shell.execute_reply": "2025-02-07T18:48:45.431020Z",
     "shell.execute_reply.started": "2025-02-07T18:48:45.424541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Printing initial probability vector\n",
    "# for key, value in initial_probability.items():\n",
    "#     print(f\"Key = {key}, Value = {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T18:48:45.433032Z",
     "iopub.status.busy": "2025-02-07T18:48:45.432844Z",
     "iopub.status.idle": "2025-02-07T18:48:45.473164Z",
     "shell.execute_reply": "2025-02-07T18:48:45.472449Z",
     "shell.execute_reply.started": "2025-02-07T18:48:45.433012Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Emission probability calculation\n",
    "emission_probability = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for key, value in word_tag_dict.items():\n",
    "    emission_probability[key[1]][key[0]] = value/tag_dict[key[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T18:48:45.474689Z",
     "iopub.status.busy": "2025-02-07T18:48:45.474271Z",
     "iopub.status.idle": "2025-02-07T18:48:45.477470Z",
     "shell.execute_reply": "2025-02-07T18:48:45.476695Z",
     "shell.execute_reply.started": "2025-02-07T18:48:45.474667Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Printing emission probability matrix\n",
    "# for tag in emission_probability.keys():\n",
    "#     for word, prob in emission_probability[tag].items():\n",
    "#         print(f\"Tag = {tag}, Word = {word}, Prob = {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T18:48:45.478387Z",
     "iopub.status.busy": "2025-02-07T18:48:45.478199Z",
     "iopub.status.idle": "2025-02-07T18:48:45.487736Z",
     "shell.execute_reply": "2025-02-07T18:48:45.486941Z",
     "shell.execute_reply.started": "2025-02-07T18:48:45.478367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Transition probability calculation\n",
    "transition_probability = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for key, value in two_tag_dict.items():\n",
    "    transition_probability[key[0]][key[1]] = value/tag_dict[key[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T18:48:45.488660Z",
     "iopub.status.busy": "2025-02-07T18:48:45.488448Z",
     "iopub.status.idle": "2025-02-07T18:48:45.496832Z",
     "shell.execute_reply": "2025-02-07T18:48:45.496019Z",
     "shell.execute_reply.started": "2025-02-07T18:48:45.488623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Viterbi algorithm\n",
    "import numpy as np\n",
    "\n",
    "def viterbi_algorithm(sentence:list, unique_tags:list, initial_prob:defaultdict, transition_prob:defaultdict, emission_prob:defaultdict):\n",
    "    n = len(sentence)  # Number of words\n",
    "    tags_list = list(unique_tags)  # Convert set to list for indexing\n",
    "\n",
    "    # Initialize Viterbi and backpointer as a list of dictionaries\n",
    "    viterbi = [{} for _ in range(n)]\n",
    "    backpointer = [{} for _ in range(n)]\n",
    "\n",
    "    # Step 1: Initialization\n",
    "    for tag in tags_list:\n",
    "        viterbi[0][tag] = initial_prob.get(tag, 0) * emission_prob.get(tag, {}).get(sentence[0], 0)\n",
    "        backpointer[0][tag] = None\n",
    "\n",
    "    # Step 2: Recursion\n",
    "    for t in range(1, n):  # Iterate over words\n",
    "        for curr_tag in tags_list:\n",
    "            max_prob, best_prev_tag = max(\n",
    "                (viterbi[t - 1].get(prev_tag, 0) * transition_prob.get(prev_tag, {}).get(curr_tag, 0) * emission_prob.get(curr_tag, {}).get(sentence[t], 0), prev_tag)\n",
    "                for prev_tag in tags_list\n",
    "            )\n",
    "            viterbi[t][curr_tag] = max_prob\n",
    "            backpointer[t][curr_tag] = best_prev_tag\n",
    "\n",
    "    # Step 3: Termination\n",
    "    best_tags = []\n",
    "    best_last_tag = max(tags_list, key=lambda tag: viterbi[-1].get(tag, 0))\n",
    "    best_tags.append(best_last_tag)\n",
    "\n",
    "    # Step 4: Backtracking\n",
    "    for t in range(n - 1, 0, -1):\n",
    "        best_last_tag = backpointer[t][best_last_tag]\n",
    "        best_tags.insert(0, best_last_tag)\n",
    "\n",
    "    return best_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T18:48:45.497868Z",
     "iopub.status.busy": "2025-02-07T18:48:45.497670Z",
     "iopub.status.idle": "2025-02-07T18:54:12.278136Z",
     "shell.execute_reply": "2025-02-07T18:54:12.276916Z",
     "shell.execute_reply.started": "2025-02-07T18:48:45.497848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare test sentences\n",
    "predicted_tags = []\n",
    "actual_tags = []\n",
    "\n",
    "# Convert string representation of lists into actual lists\n",
    "validation_sentences = []\n",
    "for _, row in test_data.iterrows():\n",
    "    try:\n",
    "        sentence = ast.literal_eval(row.iloc[0])  # Convert string to list of (word, tag) tuples\n",
    "        if isinstance(sentence, list) and len(sentence) > 0:\n",
    "            validation_sentences.append(sentence)\n",
    "    except (SyntaxError, ValueError):\n",
    "        continue  # Skip invalid rows\n",
    "\n",
    "# Check if valid sentences exist\n",
    "if not validation_sentences:\n",
    "    print(\"No valid test sentences found. Please check your dataset.\")\n",
    "    exit()\n",
    "\n",
    "# Extract list of tags\n",
    "tags_list = list(tag_dict.keys())\n",
    "\n",
    "# Run Viterbi on test sentences\n",
    "for sentence in validation_sentences:\n",
    "    words = [word for word, _ in sentence]\n",
    "    \n",
    "    if not words:  # Skip empty sentences\n",
    "        continue\n",
    "    \n",
    "    actual_tags.extend([tag for _, tag in sentence])\n",
    "    predicted_tags.extend(\n",
    "        viterbi_algorithm(\n",
    "            words,\n",
    "            tags_list,\n",
    "            initial_probability,\n",
    "            transition_probability,\n",
    "            emission_probability\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T18:54:12.279326Z",
     "iopub.status.busy": "2025-02-07T18:54:12.279072Z",
     "iopub.status.idle": "2025-02-07T18:54:14.045316Z",
     "shell.execute_reply": "2025-02-07T18:54:14.044555Z",
     "shell.execute_reply.started": "2025-02-07T18:54:12.279296Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(actual_tags, predicted_tags)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T18:54:14.046590Z",
     "iopub.status.busy": "2025-02-07T18:54:14.046338Z",
     "iopub.status.idle": "2025-02-07T18:54:18.128042Z",
     "shell.execute_reply": "2025-02-07T18:54:18.127023Z",
     "shell.execute_reply.started": "2025-02-07T18:54:14.046567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get unique tags from tag_dict\n",
    "unique_tags = list(tag_dict.keys())\n",
    "\n",
    "cm = confusion_matrix(actual_tags, predicted_tags, labels=list(unique_tags))\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    xticklabels=unique_tags,\n",
    "    yticklabels=unique_tags,\n",
    "    cmap=\"Blues\",\n",
    ")\n",
    "plt.xlabel(\"Predicted Tag\")\n",
    "plt.ylabel(\"Actual Tag\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get unique tags from tag_dict\n",
    "unique_tags = list(tag_dict.keys())\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(actual_tags, predicted_tags, labels=unique_tags)\n",
    "\n",
    "# Normalize by row (actual tag count)\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.heatmap(\n",
    "    cm_percentage,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    xticklabels=unique_tags,\n",
    "    yticklabels=unique_tags,\n",
    "    cmap=\"Blues\",\n",
    "    linewidths=0.5\n",
    ")\n",
    "plt.xlabel(\"Predicted Tag\")\n",
    "plt.ylabel(\"Actual Tag\")\n",
    "plt.title(\"Confusion Matrix (Percentage)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "datasetId": 6575580,
     "sourceId": 10620112,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
