{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T17:59:20.650358Z",
     "iopub.status.busy": "2025-02-05T17:59:20.649953Z",
     "iopub.status.idle": "2025-02-05T17:59:26.491523Z",
     "shell.execute_reply": "2025-02-05T17:59:26.490234Z",
     "shell.execute_reply.started": "2025-02-05T17:59:20.650323Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T17:59:26.493086Z",
     "iopub.status.busy": "2025-02-05T17:59:26.492830Z",
     "iopub.status.idle": "2025-02-05T18:00:20.291733Z",
     "shell.execute_reply": "2025-02-05T18:00:20.290593Z",
     "shell.execute_reply.started": "2025-02-05T17:59:26.493059Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast  # To safely evaluate string representations of lists\n",
    "\n",
    "# Load the dataset\n",
    "train_file = \"train_data.csv\"\n",
    "test_file = \"validation_data.csv\"\n",
    "\n",
    "train_data = pd.read_csv(train_file)\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "\n",
    "# import re\n",
    "# # Function to remove only the tuples containing URLs while keeping the rest of the sentence\n",
    "# def remove_tuples_with_urls(df):\n",
    "#     url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "#     return df.iloc[:, 0].astype(str).apply(lambda x: str(re.sub(url_pattern, '', x)))\n",
    "\n",
    "# # Apply the function to both datasets\n",
    "# test_data = test_data.copy()\n",
    "# train_data = train_data.copy()\n",
    "\n",
    "# test_data.iloc[:, 0] = remove_tuples_with_urls(test_data)\n",
    "# train_data.iloc[:, 0] = remove_tuples_with_urls(train_data)\n",
    "\n",
    "# # Display some cleaned samples\n",
    "# test_data.head(), train_data.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load CSV\n",
    "\n",
    "\n",
    "#Function to convert words to lowercase\n",
    "\n",
    "\n",
    "# def lowercase_and_clean(cell):\n",
    "#     try:\n",
    "#         if isinstance(cell, str):\n",
    "#             cell = ast.literal_eval(cell)  # Convert string to list of tuples\n",
    "#         if isinstance(cell, list):\n",
    "#             return [\n",
    "#                 (word.lower(), tag) \n",
    "#                 for word, tag in cell \n",
    "#                 if re.match(r'^[\\w\\s]+$', word)  # Keep only words and whitespace\n",
    "#             ]\n",
    "#         else:\n",
    "#             return cell\n",
    "#     except:\n",
    "#         return cell\n",
    "\n",
    "import re\n",
    "import ast\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lowercase_clean_lemmatize(cell):\n",
    "    try:\n",
    "        if isinstance(cell, str):\n",
    "            cell = ast.literal_eval(cell)  # Convert string to list of tuples\n",
    "        if isinstance(cell, list):\n",
    "            return [\n",
    "                (\n",
    "                    word if tag in {'PRON', 'NOUN', 'PROPN'} else lemmatizer.lemmatize(word.lower()), \n",
    "                    tag\n",
    "                )  \n",
    "                for word, tag in cell \n",
    "                if re.match(r'^[a-zA-Z\\s]+$', word)  # Keep only words and whitespace\n",
    "            ]\n",
    "        else:\n",
    "            return cell\n",
    "    except:\n",
    "        return cell\n",
    "\n",
    "\n",
    "# Apply transformation to all columns\n",
    "train_data = train_data.applymap(lowercase_clean_lemmatize)\n",
    "test_data = test_data.applymap(lowercase_clean_lemmatize)\n",
    "\n",
    "# Print result\n",
    "print(train_data.head())\n",
    "print(test_data.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:00:20.293175Z",
     "iopub.status.busy": "2025-02-05T18:00:20.292612Z",
     "iopub.status.idle": "2025-02-05T18:00:20.297871Z",
     "shell.execute_reply": "2025-02-05T18:00:20.296410Z",
     "shell.execute_reply.started": "2025-02-05T18:00:20.293148Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# import ast\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def lowercase_clean_lemmatize(cell):\n",
    "#     try:\n",
    "#         if isinstance(cell, str):\n",
    "#             cell = ast.literal_eval(cell)  # Convert string to list of tuples\n",
    "#         if isinstance(cell, list):\n",
    "#             return [\n",
    "#                 (lemmatizer.lemmatize(word.lower()), tag)  # Lowercase + Lemmatize\n",
    "#                 for word, tag in cell \n",
    "#                 if re.match(r'^[a-zA-Z\\s]+$', word)  # Keep only words and whitespace\n",
    "#             ]\n",
    "#         else:\n",
    "#             return cell\n",
    "#     except:\n",
    "#         return cell\n",
    "\n",
    "\n",
    "\n",
    "# # Apply transformation to all columns\n",
    "# train_data = train_data.applymap(lowercase_clean_lemmatize)\n",
    "# test_data = test_data.applymap(lowercase_clean_lemmatize)\n",
    "\n",
    "# # Print result\n",
    "# print(train_data.head())\n",
    "# print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:00:20.299132Z",
     "iopub.status.busy": "2025-02-05T18:00:20.298879Z",
     "iopub.status.idle": "2025-02-05T18:00:23.205138Z",
     "shell.execute_reply": "2025-02-05T18:00:23.203647Z",
     "shell.execute_reply.started": "2025-02-05T18:00:20.299108Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "\n",
    "def remove_urls(cell):\n",
    "    try:\n",
    "        if isinstance(cell, str):\n",
    "            cell = ast.literal_eval(cell)  # Convert string to list of tuples\n",
    "\n",
    "        if isinstance(cell, list):\n",
    "            return [(word, tag) for word, tag in cell if not is_url(word)]\n",
    "        else:\n",
    "            return cell\n",
    "    except:\n",
    "        return cell\n",
    "\n",
    "def is_url(word):\n",
    "    \"\"\"Check if the given word is a URL\"\"\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+|\\S+\\.(com|org|net|edu|gov|in|co|uk|us|ca|de|fr|jp|cn|au)\\b')\n",
    "    return bool(url_pattern.search(word))\n",
    "\n",
    "# Apply transformation to remove URLs\n",
    "train_data = train_data.applymap(remove_urls)\n",
    "test_data = test_data.applymap(remove_urls)\n",
    "\n",
    "# Print result\n",
    "print(train_data.head())\n",
    "print(test_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:00:23.206336Z",
     "iopub.status.busy": "2025-02-05T18:00:23.206048Z",
     "iopub.status.idle": "2025-02-05T18:00:23.210971Z",
     "shell.execute_reply": "2025-02-05T18:00:23.209300Z",
     "shell.execute_reply.started": "2025-02-05T18:00:23.206305Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:00:23.212337Z",
     "iopub.status.busy": "2025-02-05T18:00:23.212097Z",
     "iopub.status.idle": "2025-02-05T18:00:26.913155Z",
     "shell.execute_reply": "2025-02-05T18:00:26.911817Z",
     "shell.execute_reply.started": "2025-02-05T18:00:23.212313Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import ast\n",
    "\n",
    "# Dictionary of tag vs count\n",
    "tag_dict = defaultdict(int)\n",
    "\n",
    "# Dictionary of {word, tag} vs count\n",
    "word_tag_dict = defaultdict(int)\n",
    "\n",
    "# Dictionary of {tag i, tag i + 1} vs count\n",
    "two_tag_dict = defaultdict(int)\n",
    "\n",
    "# Total sentence count\n",
    "sentence_count = 0\n",
    "\n",
    "# Dictionary of first word vs count\n",
    "first_word_dict = defaultdict(int)\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "    sentence = row.iloc[0]  # Get the first column value\n",
    "    \n",
    "    # Ensure the sentence is a list, otherwise try converting\n",
    "    if isinstance(sentence, str):\n",
    "        try:\n",
    "            sentence = ast.literal_eval(sentence)\n",
    "        except (ValueError, SyntaxError):\n",
    "            print(f\"Skipping row {index}: Invalid format -> {sentence}\")\n",
    "            continue  # Skip this row\n",
    "\n",
    "    if not isinstance(sentence, list):\n",
    "        print(f\"Skipping row {index}: Not a list -> {sentence}\")\n",
    "        continue\n",
    "\n",
    "    sentence_count += 1\n",
    "\n",
    "    # Process first word\n",
    "    if len(sentence) > 0:\n",
    "        first_word_dict[(sentence[0][0], sentence[0][1])] += 1\n",
    "\n",
    "    prev_tag = None\n",
    "    for word, tag in sentence:\n",
    "        tag_dict[tag] += 1\n",
    "        word_tag_dict[(word, tag)] += 1\n",
    "\n",
    "        if prev_tag is not None:\n",
    "            two_tag_dict[(prev_tag, tag)] += 1\n",
    "        prev_tag = tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:00:26.914283Z",
     "iopub.status.busy": "2025-02-05T18:00:26.914009Z",
     "iopub.status.idle": "2025-02-05T18:00:26.919203Z",
     "shell.execute_reply": "2025-02-05T18:00:26.917906Z",
     "shell.execute_reply.started": "2025-02-05T18:00:26.914258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# import ast\n",
    "\n",
    "# # Dictionary of tag vs count\n",
    "# tag_dict = defaultdict(int)\n",
    "\n",
    "# # Dictionary of {word, tag} vs count\n",
    "# word_tag_dict = defaultdict(int)\n",
    "\n",
    "# # Dictionary of {tag i, tag i + 1} vs count\n",
    "# two_tag_dict = defaultdict(int)\n",
    "\n",
    "# # Total sentence count\n",
    "# sentence_count = 0\n",
    "\n",
    "# # Dictionary of first word vs count\n",
    "# first_word_dict = defaultdict(int)\n",
    "\n",
    "# for index, row in train_data.iterrows():\n",
    "#     sentence = ast.literal_eval(row.iloc[0])\n",
    "#     sentence_count = sentence_count + 1\n",
    "\n",
    "#     first_word_dict[(sentence[0][0], sentence[0][1])] = first_word_dict[(sentence[0][0], sentence[0][1])] + 1\n",
    "\n",
    "#     prev_tag = None\n",
    "#     for word, tag in sentence:\n",
    "#         tag_dict[tag] = tag_dict[tag] + 1\n",
    "#         word_tag_dict[(word, tag)] = word_tag_dict[(word, tag)] + 1\n",
    "\n",
    "#         if prev_tag is not None:\n",
    "#             two_tag_dict[(prev_tag, tag)] = two_tag_dict[(prev_tag, tag)] + 1\n",
    "#         prev_tag = tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:00:26.920387Z",
     "iopub.status.busy": "2025-02-05T18:00:26.920169Z",
     "iopub.status.idle": "2025-02-05T18:00:26.937016Z",
     "shell.execute_reply": "2025-02-05T18:00:26.935133Z",
     "shell.execute_reply.started": "2025-02-05T18:00:26.920364Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(tag_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:00:26.938119Z",
     "iopub.status.busy": "2025-02-05T18:00:26.937851Z",
     "iopub.status.idle": "2025-02-05T18:00:26.946959Z",
     "shell.execute_reply": "2025-02-05T18:00:26.945473Z",
     "shell.execute_reply.started": "2025-02-05T18:00:26.938093Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for key, value in word_tag_dict.items():\n",
    "#     print(f\"Key = {key}, Value = {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:00:26.948135Z",
     "iopub.status.busy": "2025-02-05T18:00:26.947873Z",
     "iopub.status.idle": "2025-02-05T18:00:26.956960Z",
     "shell.execute_reply": "2025-02-05T18:00:26.955455Z",
     "shell.execute_reply.started": "2025-02-05T18:00:26.948110Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for key, value in two_tag_dict.items():\n",
    "#     print(f\"Key = {key}, value = {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:00:26.958548Z",
     "iopub.status.busy": "2025-02-05T18:00:26.958222Z",
     "iopub.status.idle": "2025-02-05T18:00:26.966035Z",
     "shell.execute_reply": "2025-02-05T18:00:26.964906Z",
     "shell.execute_reply.started": "2025-02-05T18:00:26.958523Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for key, value in first_word_dict.items():\n",
    "#     print(f\"Key = {key}, value = {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:00:26.967685Z",
     "iopub.status.busy": "2025-02-05T18:00:26.967428Z",
     "iopub.status.idle": "2025-02-05T18:00:27.257099Z",
     "shell.execute_reply": "2025-02-05T18:00:27.255746Z",
     "shell.execute_reply.started": "2025-02-05T18:00:26.967660Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Emission probability\n",
    "emission_probability = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Transition probability\n",
    "transition_probability = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Initial probability\n",
    "initial_probability = defaultdict(int)\n",
    "\n",
    "for key, value in word_tag_dict.items():\n",
    "    emission_probability[key[0]][key[1]] = word_tag_dict[key]/tag_dict[key[1]]\n",
    "\n",
    "for key, value in two_tag_dict.items():\n",
    "    transition_probability[key[0]][key[1]] = two_tag_dict[key]/tag_dict[key[0]]\n",
    "\n",
    "for key, value in first_word_dict.items():\n",
    "    # initial_probability[key[1]] = initial_probability[key[1]] + value/sentence_count\n",
    "    initial_probability[key[1]] += value\n",
    "\n",
    "for key, value in initial_probability.items():\n",
    "    initial_probability[key] = value/sentence_count\n",
    "\n",
    "# transition_probability[\"START\"] = initial_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:00:27.258518Z",
     "iopub.status.busy": "2025-02-05T18:00:27.258264Z",
     "iopub.status.idle": "2025-02-05T18:00:27.262079Z",
     "shell.execute_reply": "2025-02-05T18:00:27.260926Z",
     "shell.execute_reply.started": "2025-02-05T18:00:27.258493Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for word in emission_probability.keys():\n",
    "#     for tag, prob in emission_probability[word].items():\n",
    "#         print(f\"Word = {word}, Tag = {tag}, Probability = {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:00:27.263054Z",
     "iopub.status.busy": "2025-02-05T18:00:27.262815Z",
     "iopub.status.idle": "2025-02-05T18:00:27.277186Z",
     "shell.execute_reply": "2025-02-05T18:00:27.275928Z",
     "shell.execute_reply.started": "2025-02-05T18:00:27.263031Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for tag_1 in transition_probability.keys():\n",
    "#     for tag_2, prob in transition_probability[tag_1].items():\n",
    "#         print(f\"Tag 1 = {tag_1}, Tag 2 = {tag_2}, Probability = {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:00:27.278687Z",
     "iopub.status.busy": "2025-02-05T18:00:27.278336Z",
     "iopub.status.idle": "2025-02-05T18:00:27.287482Z",
     "shell.execute_reply": "2025-02-05T18:00:27.286248Z",
     "shell.execute_reply.started": "2025-02-05T18:00:27.278662Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for key, value in initial_probability.items():\n",
    "#     print(f\"Tag = {key}, Probability = {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:00:27.288360Z",
     "iopub.status.busy": "2025-02-05T18:00:27.288160Z",
     "iopub.status.idle": "2025-02-05T18:00:27.298557Z",
     "shell.execute_reply": "2025-02-05T18:00:27.297232Z",
     "shell.execute_reply.started": "2025-02-05T18:00:27.288338Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Krish Attempt\n",
    "# import numpy as np\n",
    "\n",
    "# def viterbi_algorithm(\n",
    "#     sentence, unique_tags, initial_prob, transition_prob, emission_prob\n",
    "# )->list:\n",
    "#     n = len(sentence)\n",
    "#     m = len(unique_tags)\n",
    "#     tags_list = list(unique_tags)\n",
    "\n",
    "#     # Viterbi matrix\n",
    "#     viterbi = np.zeros((m, n))\n",
    "\n",
    "#     # Backpointer matrix\n",
    "#     backpointer = np.zeros((m, n), dtype=int)\n",
    "\n",
    "#     # Initialize first column\n",
    "#     for i, tag in enumerate(tags_list):\n",
    "#         viterbi[i, 0] = initial_prob.get(tag, 1e-6) * emission_prob.get(\n",
    "#             (tag, sentence[0]), 1e-6\n",
    "#         )\n",
    "\n",
    "#     # Recursion step\n",
    "#     for t in range(1, n):\n",
    "#         for j, curr_tag in enumerate(tags_list):\n",
    "#             max_prob, best_prev_tag = max(\n",
    "#                 [\n",
    "#                     (\n",
    "#                         viterbi[i, t - 1]\n",
    "#                         * transition_prob.get((prev_tag, curr_tag), 1e-6)\n",
    "#                         * emission_prob.get((curr_tag, sentence[t]), 1e-6),\n",
    "#                         i,\n",
    "#                     )\n",
    "#                     for i, prev_tag in enumerate(tags_list)\n",
    "#                 ]\n",
    "#             )\n",
    "#             viterbi[j, t] = max_prob\n",
    "#             backpointer[j, t] = best_prev_tag\n",
    "\n",
    "#     # Backtracking to retrieve the best sequence\n",
    "#     best_tags = []\n",
    "#     best_last_tag = np.argmax(viterbi[:, n - 1])\n",
    "#     best_tags.append(tags_list[best_last_tag])\n",
    "\n",
    "#     for t in range(n - 1, 0, -1):\n",
    "#         best_last_tag = backpointer[best_last_tag, t]\n",
    "#         best_tags.insert(0, tags_list[best_last_tag])\n",
    "\n",
    "#     return best_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:00:27.299408Z",
     "iopub.status.busy": "2025-02-05T18:00:27.299203Z",
     "iopub.status.idle": "2025-02-05T18:00:27.309191Z",
     "shell.execute_reply": "2025-02-05T18:00:27.308398Z",
     "shell.execute_reply.started": "2025-02-05T18:00:27.299386Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Valmik attempt - Optimized for efficiency using list of dictionaries\n",
    "\n",
    "# def viterbi_algorithm(sentence, unique_tags, initial_prob, transition_prob, emission_prob) -> list:\n",
    "#     n = len(sentence)  # Number of words\n",
    "#     tags_list = list(unique_tags)  # Convert set to list for indexing\n",
    "    \n",
    "#     # Using list of dictionaries for better performance\n",
    "#     viterbi = [{} for _ in range(n)]  # List of dictionaries for Viterbi matrix\n",
    "#     backpointer = [{} for _ in range(n)]  # List of dictionaries for backpointer matrix\n",
    "\n",
    "#     # Initialize first column\n",
    "#     for tag in tags_list:\n",
    "#         viterbi[0][tag] = initial_prob.get(tag, 1e-6) * emission_prob.get((tag, sentence[0]), 1e-6)\n",
    "#         backpointer[0][tag] = None\n",
    "\n",
    "#     # Recursion step using list of dictionaries\n",
    "#     for t in range(1, n):\n",
    "#         for curr_tag in tags_list:\n",
    "#             max_prob, best_prev_tag = max(\n",
    "#                 (\n",
    "#                     viterbi[t - 1].get(prev_tag, 1e-6) * transition_prob.get((prev_tag, curr_tag), 1e-6) * emission_prob.get((curr_tag, sentence[t]), 1e-6),\n",
    "#                     prev_tag\n",
    "#                 )\n",
    "#                 for prev_tag in tags_list\n",
    "#             )\n",
    "#             viterbi[t][curr_tag] = max_prob\n",
    "#             backpointer[t][curr_tag] = best_prev_tag\n",
    "\n",
    "#     # Backtracking to retrieve the best sequence\n",
    "#     best_tags = []\n",
    "#     best_last_tag = max(tags_list, key=lambda tag: viterbi[-1].get(tag, 0))\n",
    "#     best_tags.append(best_last_tag)\n",
    "\n",
    "#     for t in range(n - 1, 0, -1):\n",
    "#         best_last_tag = backpointer[t][best_last_tag]\n",
    "#         best_tags.insert(0, best_last_tag)\n",
    "\n",
    "#     return best_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:00:27.310885Z",
     "iopub.status.busy": "2025-02-05T18:00:27.310657Z",
     "iopub.status.idle": "2025-02-05T18:00:27.324027Z",
     "shell.execute_reply": "2025-02-05T18:00:27.323227Z",
     "shell.execute_reply.started": "2025-02-05T18:00:27.310861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# #Aditya Attempt\n",
    "# import numpy as np\n",
    "# import ast\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# def viterbi_algorithm(sentence, unique_tags, initial_prob, transition_prob, emission_prob):\n",
    "#     n = len(sentence)\n",
    "#     tags_list = list(unique_tags)\n",
    "#     num_tags = len(tags_list)\n",
    "#     tag_index = {tag: i for i, tag in enumerate(tags_list)}\n",
    "\n",
    "#     # Use better smoothing for missing probabilities\n",
    "#     log_initial_prob = np.log(np.array([initial_prob.get(tag, 1e-5) for tag in tags_list]))  \n",
    "#     log_transition_prob = np.log(np.array([[transition_prob.get((prev, curr), 1e-5) for curr in tags_list] for prev in tags_list]))\n",
    "\n",
    "#     # Emission probability matrix (with Laplace smoothing)\n",
    "#     log_emission_prob = np.full((num_tags, n), -np.inf)\n",
    "#     for i, word in enumerate(sentence):\n",
    "#         for tag in tags_list:\n",
    "#             prob = emission_prob.get((tag, word), 1e-5)  # Handle unseen words\n",
    "#             log_emission_prob[tag_index[tag], i] = np.log(prob)\n",
    "\n",
    "#     # Initialize Viterbi table\n",
    "#     viterbi = np.full((num_tags, n), -np.inf)\n",
    "#     backpointer = np.zeros((num_tags, n), dtype=int)\n",
    "\n",
    "#     # Initialization step\n",
    "#     viterbi[:, 0] = log_initial_prob + log_emission_prob[:, 0]\n",
    "\n",
    "#     # Recursion step\n",
    "#     for t in range(1, n):\n",
    "#         log_probs = viterbi[:, t-1][:, None] + log_transition_prob + log_emission_prob[:, t]\n",
    "#         viterbi[:, t] = np.max(log_probs, axis=0)\n",
    "#         backpointer[:, t] = np.argmax(log_probs, axis=0)\n",
    "\n",
    "#     # Backtracking step\n",
    "#     best_tags = []\n",
    "#     best_last_idx = np.argmax(viterbi[:, -1])\n",
    "#     best_tags.append(tags_list[best_last_idx])\n",
    "\n",
    "#     for t in range(n - 1, 0, -1):\n",
    "#         best_last_idx = backpointer[best_last_idx, t]\n",
    "#         best_tags.insert(0, tags_list[best_last_idx])\n",
    "\n",
    "#     return best_tags\n",
    "\n",
    "# # Prepare test sentences\n",
    "# predicted_tags = []\n",
    "# actual_tags = []\n",
    "# validation_sentences = [ast.literal_eval(row.iloc[0]) for _, row in test_data.iterrows()]\n",
    "\n",
    "# tags_list = list(tag_dict.keys())\n",
    "\n",
    "# # Ensure emission probability is correctly structured\n",
    "# temp_emission_prob = {\n",
    "#     (tag, word): emission_probability[word].get(tag, 1e-5) for word in emission_probability for tag in tags_list\n",
    "# }\n",
    "\n",
    "# # Run Viterbi on test sentences\n",
    "# for sentence in validation_sentences:\n",
    "#     words = [word for word, _ in sentence]\n",
    "#     actual_tags.extend([tag for _, tag in sentence])\n",
    "#     predicted_tags.extend(\n",
    "#         viterbi_algorithm(\n",
    "#             words,\n",
    "#             tags_list,\n",
    "#             initial_probability,\n",
    "#             transition_probability,\n",
    "#             temp_emission_prob\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# # Evaluate accuracy\n",
    "# accuracy = accuracy_score(actual_tags, predicted_tags)\n",
    "# print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:00:27.325169Z",
     "iopub.status.busy": "2025-02-05T18:00:27.324950Z",
     "iopub.status.idle": "2025-02-05T18:00:27.339054Z",
     "shell.execute_reply": "2025-02-05T18:00:27.337828Z",
     "shell.execute_reply.started": "2025-02-05T18:00:27.325145Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# def viterbi_algorithm(sentence, unique_tags, initial_prob, transition_prob, emission_prob):\n",
    "#     n = len(sentence)\n",
    "#     tags_list = list(unique_tags)\n",
    "#     num_tags = len(tags_list)\n",
    "#     tag_index = {tag: i for i, tag in enumerate(tags_list)}\n",
    "\n",
    "#     # Use log probabilities for numerical stability (with smoothing)\n",
    "#     log_initial_prob = np.log(np.array([initial_prob.get(tag, 1e-5) for tag in tags_list]))  \n",
    "#     log_transition_prob = np.log(np.array([[transition_prob.get((prev, curr), 1e-5) for curr in tags_list] for prev in tags_list]))\n",
    "\n",
    "#     # Emission probability matrix (with Laplace smoothing)\n",
    "#     log_emission_prob = np.full((num_tags, n), -np.inf)\n",
    "#     for i, word in enumerate(sentence):\n",
    "#         for tag in tags_list:\n",
    "#             prob = emission_prob.get((tag, word), 1e-5)  # Handle unseen words\n",
    "#             log_emission_prob[tag_index[tag], i] = np.log(prob)\n",
    "\n",
    "#     # Initialize Viterbi table\n",
    "#     viterbi = np.full((num_tags, n), -np.inf)\n",
    "#     backpointer = np.zeros((num_tags, n), dtype=int)\n",
    "\n",
    "#     # Initialization step\n",
    "#     viterbi[:, 0] = log_initial_prob + log_emission_prob[:, 0]\n",
    "\n",
    "#     # Recursion step\n",
    "#     for t in range(1, n):\n",
    "#         log_probs = viterbi[:, t-1][:, None] + log_transition_prob + log_emission_prob[:, t]\n",
    "#         viterbi[:, t] = np.max(log_probs, axis=0)\n",
    "#         backpointer[:, t] = np.argmax(log_probs, axis=0)\n",
    "\n",
    "#     # Backtracking step\n",
    "#     best_tags = []\n",
    "#     best_last_idx = np.argmax(viterbi[:, -1])\n",
    "#     best_tags.append(tags_list[best_last_idx])\n",
    "\n",
    "#     for t in range(n - 1, 0, -1):\n",
    "#         best_last_idx = backpointer[best_last_idx, t]\n",
    "#         best_tags.insert(0, tags_list[best_last_idx])\n",
    "\n",
    "#     return best_tags\n",
    "\n",
    "\n",
    "# # Prepare test sentences\n",
    "# predicted_tags = []\n",
    "# actual_tags = []\n",
    "\n",
    "# # Assuming test_data is a DataFrame, check its first few rows\n",
    "# # If test_data is a CSV, read it correctly\n",
    "# # print(test_data.head())  # Uncomment this line to debug\n",
    "\n",
    "# # Directly use the valid sentences\n",
    "# validation_sentences = [\n",
    "#     row.iloc[0] for _, row in test_data.iterrows() if isinstance(row.iloc[0], list)  # Ensure it's a list\n",
    "# ]\n",
    "\n",
    "# # Check if there are any valid sentences\n",
    "# if not validation_sentences:\n",
    "#     print(\"No valid test sentences found. Please check your dataset.\")\n",
    "#     exit()\n",
    "\n",
    "# tags_list = list(tag_dict.keys())\n",
    "\n",
    "# # Ensure emission probability is structured correctly\n",
    "# temp_emission_prob = {\n",
    "#     (tag, word): emission_probability.get(word, {}).get(tag, 1e-5) \n",
    "#     for word in emission_probability \n",
    "#     for tag in tags_list\n",
    "# }\n",
    "\n",
    "# # Run Viterbi on test sentences\n",
    "# for sentence in validation_sentences:\n",
    "#     words = [word for word, _ in sentence]\n",
    "#     actual_tags.extend([tag for _, tag in sentence])\n",
    "#     predicted_tags.extend(\n",
    "#         viterbi_algorithm(\n",
    "#             words,\n",
    "#             tags_list,\n",
    "#             initial_probability,\n",
    "#             transition_probability,\n",
    "#             temp_emission_prob\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# # Evaluate accuracy\n",
    "# accuracy = accuracy_score(actual_tags, predicted_tags)\n",
    "# print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:00:27.340055Z",
     "iopub.status.busy": "2025-02-05T18:00:27.339808Z",
     "iopub.status.idle": "2025-02-05T18:01:42.861333Z",
     "shell.execute_reply": "2025-02-05T18:01:42.859983Z",
     "shell.execute_reply.started": "2025-02-05T18:00:27.340030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to check if a word is a URL\n",
    "# def is_url(word):\n",
    "#     url_pattern = re.compile(r'https?://\\S+|www\\.\\S+|\\S+\\.(com|org|net|edu|gov|in|co|uk|us|ca|de|fr|jp|cn|au)\\b')\n",
    "#     return bool(url_pattern.search(word))\n",
    "\n",
    "# # Function to remove URLs from tagged sentence tuples\n",
    "# def remove_urls(cell):\n",
    "#     try:\n",
    "#         if isinstance(cell, str):\n",
    "#             cell = ast.literal_eval(cell)  # Convert string to list of tuples\n",
    "#         if isinstance(cell, list):\n",
    "#             return [(word, tag) for word, tag in cell if not is_url(word)]  # Remove URL tuples\n",
    "#         else:\n",
    "#             return cell\n",
    "#     except:\n",
    "#         return cell\n",
    "\n",
    "# # Apply the function to remove URLs from dataset\n",
    "# train_data = train_data.applymap(remove_urls)\n",
    "# test_data = test_data.applymap(remove_urls)\n",
    "\n",
    "# Viterbi Algorithm Implementation\n",
    "def viterbi_algorithm(sentence, unique_tags, initial_prob, transition_prob, emission_prob):\n",
    "    if not sentence:  # Check if sentence is empty\n",
    "        return []\n",
    "\n",
    "    n = len(sentence)\n",
    "    tags_list = list(unique_tags)\n",
    "    num_tags = len(tags_list)\n",
    "    tag_index = {tag: i for i, tag in enumerate(tags_list)}\n",
    "\n",
    "    # Use log probabilities for numerical stability (with smoothing)\n",
    "    log_initial_prob = np.log(np.array([initial_prob.get(tag, 1e-100) for tag in tags_list]))  \n",
    "    log_transition_prob = np.log(np.array([[transition_prob.get((prev, curr), 1e-100) for curr in tags_list] for prev in tags_list]))\n",
    "\n",
    "    # Emission probability matrix (with Laplace smoothing)\n",
    "    log_emission_prob = np.full((num_tags, n), -np.inf)\n",
    "    for i, word in enumerate(sentence):\n",
    "        for tag in tags_list:\n",
    "            prob = emission_prob.get((tag, word), 1e-100)  # Handle unseen words\n",
    "            log_emission_prob[tag_index[tag], i] = np.log(prob)\n",
    "\n",
    "    # Initialize Viterbi table\n",
    "    viterbi = np.full((num_tags, n), -np.inf)\n",
    "    backpointer = np.zeros((num_tags, n), dtype=int)\n",
    "\n",
    "    # Initialization step\n",
    "    viterbi[:, 0] = log_initial_prob + log_emission_prob[:, 0]\n",
    "\n",
    "    # Recursion step\n",
    "    for t in range(1, n):\n",
    "        log_probs = viterbi[:, t-1][:, None] + log_transition_prob + log_emission_prob[:, t]\n",
    "        viterbi[:, t] = np.max(log_probs, axis=0)\n",
    "        backpointer[:, t] = np.argmax(log_probs, axis=0)\n",
    "\n",
    "    # Backtracking step\n",
    "    best_tags = []\n",
    "    best_last_idx = np.argmax(viterbi[:, -1])\n",
    "    best_tags.append(tags_list[best_last_idx])\n",
    "\n",
    "    for t in range(n - 1, 0, -1):\n",
    "        best_last_idx = backpointer[best_last_idx, t]\n",
    "        best_tags.insert(0, tags_list[best_last_idx])\n",
    "\n",
    "    return best_tags\n",
    "\n",
    "\n",
    "# Prepare test sentences\n",
    "predicted_tags = []\n",
    "actual_tags = []\n",
    "\n",
    "# Get valid sentences after URL removal\n",
    "validation_sentences = [\n",
    "    row.iloc[0] for _, row in test_data.iterrows() if isinstance(row.iloc[0], list) and len(row.iloc[0]) > 0  # Check for non-empty lists\n",
    "]\n",
    "\n",
    "# Check if valid sentences exist\n",
    "if not validation_sentences:\n",
    "    print(\"No valid test sentences found. Please check your dataset.\")\n",
    "    exit()\n",
    "\n",
    "tags_list = list(tag_dict.keys())\n",
    "\n",
    "# Ensure emission probability is structured correctly\n",
    "temp_emission_prob = {\n",
    "    (tag, word): emission_probability.get(word, {}).get(tag, 1e-100) \n",
    "    for word in emission_probability \n",
    "    for tag in tags_list\n",
    "}\n",
    "\n",
    "# Run Viterbi on test sentences\n",
    "for sentence in validation_sentences:\n",
    "    words = [word for word, _ in sentence]\n",
    "    \n",
    "    if not words:  # Skip empty sentences\n",
    "        continue\n",
    "    \n",
    "    actual_tags.extend([tag for _, tag in sentence])\n",
    "    predicted_tags.extend(\n",
    "        viterbi_algorithm(\n",
    "            words,\n",
    "            tags_list,\n",
    "            initial_probability,\n",
    "            transition_probability,\n",
    "            temp_emission_prob\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(actual_tags, predicted_tags)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:01:42.862492Z",
     "iopub.status.busy": "2025-02-05T18:01:42.862231Z",
     "iopub.status.idle": "2025-02-05T18:01:42.866380Z",
     "shell.execute_reply": "2025-02-05T18:01:42.865634Z",
     "shell.execute_reply.started": "2025-02-05T18:01:42.862465Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "# # Optimized POS Tag Prediction for Validation Dataset\n",
    "# predicted_tags = []\n",
    "# actual_tags = []\n",
    "# validation_sentences = []\n",
    "\n",
    "# # Preprocess validation sentences\n",
    "# validation_sentences = [ast.literal_eval(row.iloc[0]) for _, row in test_data.iterrows()]\n",
    "\n",
    "# # Convert emission_probability to match expected format once to avoid redundant computation\n",
    "# emission_prob = {\n",
    "#     (tag, word): prob\n",
    "#     for word, tags in emission_probability.items()\n",
    "#     for tag, prob in tags.items()\n",
    "# }\n",
    "\n",
    "# def process_sentence(sentence):\n",
    "#     words = [word for word, _ in sentence]\n",
    "#     actual_tags.extend([tag for _, tag in sentence])  # Storing actual tags\n",
    "#     return viterbi_algorithm(\n",
    "#         words,\n",
    "#         tag_dict.keys(),\n",
    "#         initial_probability,\n",
    "#         transition_probability,\n",
    "#         emission_prob,\n",
    "#     )\n",
    "\n",
    "# # Process all sentences efficiently using list comprehension\n",
    "# predicted_tags_nested = [process_sentence(sentence) for sentence in validation_sentences]\n",
    "\n",
    "# # Flatten predicted tags list\n",
    "# predicted_tags = list(itertools.chain.from_iterable(predicted_tags_nested))\n",
    "\n",
    "# # Ensure lengths match\n",
    "# assert len(actual_tags) == len(predicted_tags), \"Mismatch between actual and predicted tag lengths!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:01:42.867708Z",
     "iopub.status.busy": "2025-02-05T18:01:42.867252Z",
     "iopub.status.idle": "2025-02-05T18:01:42.885692Z",
     "shell.execute_reply": "2025-02-05T18:01:42.884881Z",
     "shell.execute_reply.started": "2025-02-05T18:01:42.867685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# accuracy = accuracy_score(actual_tags, predicted_tags)\n",
    "# print(f\"Model Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T18:01:42.886726Z",
     "iopub.status.busy": "2025-02-05T18:01:42.886498Z",
     "iopub.status.idle": "2025-02-05T18:01:48.409464Z",
     "shell.execute_reply": "2025-02-05T18:01:48.408599Z",
     "shell.execute_reply.started": "2025-02-05T18:01:42.886704Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get unique tags from tag_dict\n",
    "unique_tags = list(tag_dict.keys())\n",
    "\n",
    "cm = confusion_matrix(actual_tags, predicted_tags, labels=list(unique_tags))\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    xticklabels=unique_tags,\n",
    "    yticklabels=unique_tags,\n",
    "    cmap=\"Blues\",\n",
    ")\n",
    "plt.xlabel(\"Predicted Tag\")\n",
    "plt.ylabel(\"Actual Tag\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "datasetId": 6575580,
     "sourceId": 10620112,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
